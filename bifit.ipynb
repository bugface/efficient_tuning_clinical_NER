{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automatic-today",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HTTP_PROXY']=\"http://proxe.shands.ufl.edu:3128\"\n",
    "os.environ['HTTPS_PROXY']=\"http://proxe.shands.ufl.edu:3128\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0,1,2,3,4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-purpose",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/alexgre/workspace/py3/nlp/ClinicalTransformerNER/\")\n",
    "sys.path.append(\"/home/alexgre/workspace/py3/nlp/ClinicalTransformerNER/src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-sculpture",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from transformer_ner.transfomer_log import TransformerNERLogger\n",
    "from transformer_ner.model import MegatronNerModel, BertConfig\n",
    "from transformer_ner.task import train, predict, evaluate, _output_bio, set_seed, set_up_eval_tool, load_model\n",
    "from transformers import BertTokenizer\n",
    "from transformer_ner.data_utils import (NEXT_GUARD, NEXT_TOKEN,\n",
    "                                        TransformerNerDataProcessor,\n",
    "                                        batch_to_model_inputs,\n",
    "                                        convert_features_to_tensors,\n",
    "                                        ner_data_loader,\n",
    "                                        transformer_convert_data_to_features)\n",
    "from common_utils.common_io import json_dump, json_load, output_bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chinese-memorial",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BIAS_TERMS_DICT = {\n",
    "    'intermediate': 'intermediate.dense.bias',\n",
    "    'key': 'attention.self.key.bias',\n",
    "    'query': 'attention.self.query.bias',\n",
    "    'value': 'attention.self.value.bias',\n",
    "    'output': 'output.dense.bias',\n",
    "    'output_layernorm': 'output.LayerNorm.bias',\n",
    "    'attention_layernorm': 'attention.output.LayerNorm.bias',\n",
    "    'all': 'bias',\n",
    "}\n",
    "\n",
    "classifier_name = \"classifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-chaos",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plm = \"/home/alexgre/projects/transformer_pretrained_models/gatortron-syn-345m_deid_vocab/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-ability",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainale_params = list(BIAS_TERMS_DICT.values()) + ['pooler.dense.bias'] + [classifier_name]\n",
    "\n",
    "\n",
    "def set_freeze(model):\n",
    "    for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "    for name, param in model.named_parameters():\n",
    "        for tp in trainale_params:\n",
    "            if tp in name:\n",
    "                param.requires_grad = True\n",
    "                break\n",
    "\n",
    "\n",
    "def set_unfreeze(model):\n",
    "    for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "\n",
    "def count_param(model):\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "    pytorch_train_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    pre = round(pytorch_train_params / pytorch_total_params, 4) * 100\n",
    "    print(f\"\"\"\n",
    "    total number parameters: {pytorch_total_params}\n",
    "    total trainable number parameters: {pytorch_train_params}\n",
    "    precetage: {pre}%\n",
    "    \"\"\")\n",
    "            \n",
    "# set_freeze(model)\n",
    "# set_unfreeze(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-basis",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     model.classifier.weight.copy_(model1.classifier.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-despite",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(\n",
    "        self, model_type, pretrained_model, \n",
    "        do_train=True, do_predict=True,\n",
    "        new_model_dir=None, resume_from_model=None, \n",
    "        data_dir=None, logger_file=\"log.txt\"\n",
    "    ):\n",
    "        self.model_type = model_type\n",
    "        self.pretrained_model = pretrained_model if resume_from_model is None else resume_from_model\n",
    "        self.config_name = self.pretrained_model\n",
    "        self.tokenizer_name = self.pretrained_model\n",
    "        self.do_lower_case = True\n",
    "        self.overwrite_model_dir = True\n",
    "        self.data_dir = data_dir\n",
    "        self.data_has_offset_information = False\n",
    "        self.new_model_dir = new_model_dir\n",
    "        self.predict_output_file = Path(new_model_dir) / \"predicted.txt\"\n",
    "        self.overwrite_output_dir = True\n",
    "        self.max_seq_length = 512\n",
    "        self.do_train = do_train\n",
    "        self.do_predict = do_predict\n",
    "        self.model_selection_scoring = \"strict-f_score-1\"\n",
    "        self.train_batch_size = 4\n",
    "        self.eval_batch_size = 32\n",
    "        self.learning_rate = 2e-5\n",
    "        self.min_lr = 5e-6\n",
    "        self.seed = 42\n",
    "        self.logger = TransformerNERLogger(\n",
    "            logger_level=\"i\",\n",
    "            logger_file=logger_file).get_logger()\n",
    "        self.num_train_epochs = 30\n",
    "        self.gradient_accumulation_steps = 1\n",
    "        self.do_warmup = True\n",
    "        self.label2idx = None\n",
    "        self.idx2label = None\n",
    "        self.max_num_checkpoints = 2\n",
    "        self.warmup_ratio = 0.1\n",
    "        self.weight_decay = 0.01\n",
    "        self.adam_epsilon = 0.00000001\n",
    "        self.max_grad_norm = 1.0\n",
    "        self.log_file = None\n",
    "        self.log_lvl = None\n",
    "        self.fp16 = False\n",
    "        self.local_rank = -1\n",
    "        self.device = \"cpu\"\n",
    "        self.train_steps = 2000\n",
    "        self.early_stop = 3\n",
    "        self.progress_bar = False\n",
    "        self.save_model_core = True\n",
    "        self.use_crf = False\n",
    "        self.focal_loss = False\n",
    "        self.focal_loss_gamma = 2\n",
    "        self.resume_from_model = resume_from_model\n",
    "        self.use_biaffine = False\n",
    "        self.mlp_dim = 128\n",
    "        self.mlp_layers = 0\n",
    "        self.adversarial_training = False\n",
    "        self.adversarial_training_method = None  # None, \"fgm\", \"pgd\", \"freelb\n",
    "        self.adversarial_training_conf = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integrated-writing",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2010 i2b2\n",
    "\n",
    "# i2b2_2010 = \"/home/alexgre/shared_data/challenge_datasets/ner/clinical_ner_datasets/2010_i2b2/\"\n",
    "i2b2_2010 = \"/home/alexgre/shared_data/challenge_datasets/ner/clinical_ner_datasets/2012_i2b2/preprocessed_bio_data/\"\n",
    "\n",
    "args1 = Args(\n",
    "    model_type=\"megatron\", \n",
    "    pretrained_model=None, \n",
    "    new_model_dir=\"./i2b2_2012_megatron_bitfit_stage1\", \n",
    "    data_dir=i2b2_2010,\n",
    "    do_predict = False,\n",
    "    logger_file=\"i2b2_2012_megatron_bitfit_stage1.txt\" \n",
    ")\n",
    "\n",
    "args1.device = torch.device(\"cuda:4\")\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(plm, do_lower_case=args1.do_lower_case, add_prefix_space=True)\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": [NEXT_TOKEN]})\n",
    "new_dim = len(tokenizer)\n",
    "\n",
    "set_seed(args1.seed)\n",
    "\n",
    "ner_data_processor = TransformerNerDataProcessor()\n",
    "ner_data_processor.set_data_dir(args1.data_dir)\n",
    "ner_data_processor.set_logger(args1.logger)\n",
    "#     ner_data_processor.offset_info_available()\n",
    "\n",
    "labels, label2idx = ner_data_processor.get_labels(default=args1.model_type)\n",
    "num_labels = len(label2idx)\n",
    "idx2label = {v: k for k, v in label2idx.items()}\n",
    "\n",
    "args1.num_labels = num_labels\n",
    "args1.label2idx = label2idx\n",
    "args1.idx2label = idx2label\n",
    "\n",
    "\n",
    "\n",
    "train_examples = ner_data_processor.get_train_examples()\n",
    "train_features = transformer_convert_data_to_features(args1,\n",
    "                                                      input_examples=train_examples,\n",
    "                                                      label2idx=label2idx,\n",
    "                                                      tokenizer=tokenizer,\n",
    "                                                      max_seq_len=args1.max_seq_length)\n",
    "\n",
    "dev_examples = ner_data_processor.get_dev_examples()\n",
    "dev_features = transformer_convert_data_to_features(args1,\n",
    "                                                    input_examples=dev_examples,\n",
    "                                                    label2idx=label2idx,\n",
    "                                                    tokenizer=tokenizer,\n",
    "                                                    max_seq_len=args1.max_seq_length)\n",
    "\n",
    "args1.eval_tool = set_up_eval_tool(args1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-fields",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bitfit\n",
    "conf_bitfit = BertConfig.from_pretrained(plm, num_labels=num_labels)\n",
    "model_bitfit = MegatronNerModel.from_pretrained(plm, config=conf_bitfit)\n",
    "model_bitfit.resize_token_embeddings(new_dim)\n",
    "conf_bitfit.vocab_size = new_dim\n",
    "args1.config = model_bitfit.config\n",
    "args1.tokenizer = tokenizer\n",
    "set_freeze(model_bitfit)\n",
    "count_param(model_bitfit)\n",
    "model_bitfit.to(args1.device)\n",
    "# start training\n",
    "train(args1, model_bitfit, train_features, dev_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-louis",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_example = ner_data_processor.get_test_examples()\n",
    "test_features = transformer_convert_data_to_features(args1,\n",
    "                                                     input_examples=test_example,\n",
    "                                                     label2idx=label2idx,\n",
    "                                                     tokenizer=args1.tokenizer,\n",
    "                                                     max_seq_len=args1.max_seq_length)\n",
    "\n",
    "model = load_model(args1)\n",
    "model.to(args1.device)\n",
    "\n",
    "predictions = predict(args1, model, test_features)\n",
    "_output_bio(args1, test_example, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-madison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval\n",
    "! python3 \"/home/alexgre/workspace/py3/nlp/ClinicalTransformerNER/src/eval_scripts/new_bio_eval.py\"\\\n",
    "    -f1 /home/alexgre/shared_data/challenge_datasets/ner/clinical_ner_datasets/2012_i2b2/preprocessed_bio_data/test.txt \\\n",
    "    -f2 \"./i2b2_2012_megatron_bitfit_stage1/predicted.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-andrews",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load bitfit model and get classifier only\n",
    "temp_model = load_model(args1)\n",
    "\n",
    "args2 = Args(\n",
    "    model_type=\"megatron\", \n",
    "    pretrained_model=None, \n",
    "    new_model_dir=\"./i2b2_2012_megatron_bitfit_stage2\", \n",
    "    data_dir=i2b2_2010,\n",
    "    do_predict = True,\n",
    "    logger_file=\"i2b2_2012__megatron_bitfit_stage2.txt\" \n",
    ")\n",
    "args2.device = torch.device(\"cuda:4\")\n",
    "args2.num_labels = num_labels\n",
    "args2.label2idx = label2idx\n",
    "args2.idx2label = idx2label\n",
    "args2.eval_tool = set_up_eval_tool(args2)\n",
    "args2.learning_rate = 1e-5\n",
    "args2.seed = 13\n",
    "\n",
    "train_examples = ner_data_processor.get_train_examples()\n",
    "train_features = transformer_convert_data_to_features(args2,\n",
    "                                                      input_examples=train_examples,\n",
    "                                                      label2idx=label2idx,\n",
    "                                                      tokenizer=tokenizer,\n",
    "                                                      max_seq_len=args2.max_seq_length)\n",
    "\n",
    "dev_examples = ner_data_processor.get_dev_examples()\n",
    "dev_features = transformer_convert_data_to_features(args2,\n",
    "                                                    input_examples=dev_examples,\n",
    "                                                    label2idx=label2idx,\n",
    "                                                    tokenizer=tokenizer,\n",
    "                                                    max_seq_len=args2.max_seq_length)\n",
    "\n",
    "# full FT\n",
    "conf_bitfit_ft = BertConfig.from_pretrained(plm, num_labels=num_labels)\n",
    "model_bitfit_ft = MegatronNerModel.from_pretrained(plm, config=conf_bitfit_ft)\n",
    "model_bitfit_ft.resize_token_embeddings(new_dim)\n",
    "conf_bitfit_ft.vocab_size = new_dim\n",
    "args2.config = model_bitfit_ft.config\n",
    "args2.tokenizer = tokenizer\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_bitfit_ft.classifier.weight.copy_(temp_model.classifier.weight)\n",
    "    model_bitfit_ft.classifier.bias.copy_(temp_model.classifier.bias)\n",
    "\n",
    "count_param(model_bitfit_ft)    \n",
    "\n",
    "model_bitfit_ft.to(args2.device)\n",
    "# start training\n",
    "train(args2, model_bitfit_ft, train_features, dev_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-syria",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_example = ner_data_processor.get_test_examples()\n",
    "test_features = transformer_convert_data_to_features(args2,\n",
    "                                                     input_examples=test_example,\n",
    "                                                     label2idx=label2idx,\n",
    "                                                     tokenizer=args2.tokenizer,\n",
    "                                                     max_seq_len=args2.max_seq_length)\n",
    "\n",
    "model = load_model(args2)\n",
    "model.to(args2.device)\n",
    "\n",
    "predictions = predict(args2, model, test_features)\n",
    "_output_bio(args2, test_example, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-suggestion",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# eval\n",
    "! python3 \"/home/alexgre/workspace/py3/nlp/ClinicalTransformerNER/src/eval_scripts/new_bio_eval.py\"\\\n",
    "    -f1 /home/alexgre/shared_data/challenge_datasets/ner/clinical_ner_datasets/2012_i2b2/preprocessed_bio_data/test.txt \\\n",
    "    -f2 \"./i2b2_2012_megatron_bitfit_stage2/predicted.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-services",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bitfit reserve\n",
    "\n",
    "args2 = Args(\n",
    "    model_type=\"megatron\", \n",
    "    pretrained_model=None, \n",
    "    new_model_dir=\"./i2b2_2012_megatron_bitfit_stage2_reserve\", \n",
    "    data_dir=i2b2_2010,\n",
    "    do_predict = True,\n",
    "    logger_file=\"i2b2_2012_megatron_bitfit_stage2_reserve.txt\" \n",
    ")\n",
    "args2.device = torch.device(\"cuda:4\")\n",
    "args2.num_labels = num_labels\n",
    "args2.label2idx = label2idx\n",
    "args2.idx2label = idx2label\n",
    "args2.eval_tool = set_up_eval_tool(args2)\n",
    "args2.learning_rate = 1e-5\n",
    "args2.seed = 13\n",
    "\n",
    "train_examples = ner_data_processor.get_train_examples()\n",
    "train_features = transformer_convert_data_to_features(args2,\n",
    "                                                      input_examples=train_examples,\n",
    "                                                      label2idx=label2idx,\n",
    "                                                      tokenizer=tokenizer,\n",
    "                                                      max_seq_len=args2.max_seq_length)\n",
    "\n",
    "dev_examples = ner_data_processor.get_dev_examples()\n",
    "dev_features = transformer_convert_data_to_features(args2,\n",
    "                                                    input_examples=dev_examples,\n",
    "                                                    label2idx=label2idx,\n",
    "                                                    tokenizer=tokenizer,\n",
    "                                                    max_seq_len=args2.max_seq_length)\n",
    "\n",
    "# full FT\n",
    "conf_bitfit_ft = BertConfig.from_pretrained(args1.new_model_dir)\n",
    "model_bitfit_ft = load_model(args1)\n",
    "# model_bitfit_ft.resize_token_embeddings(new_dim)\n",
    "# conf_bitfit_ft.vocab_size = new_dim\n",
    "args2.config = model_bitfit_ft.config\n",
    "args2.tokenizer = tokenizer\n",
    "\n",
    "count_param(model_bitfit_ft)    \n",
    "\n",
    "model_bitfit_ft.to(args2.device)\n",
    "# start training\n",
    "train(args2, model_bitfit_ft, train_features, dev_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiet-leather",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(args2.__dict__)\n",
    "test_example = ner_data_processor.get_test_examples()\n",
    "test_features = transformer_convert_data_to_features(args2,\n",
    "                                                     input_examples=test_example,\n",
    "                                                     label2idx=label2idx,\n",
    "                                                     tokenizer=args2.tokenizer,\n",
    "                                                     max_seq_len=args2.max_seq_length)\n",
    "\n",
    "model = load_model(args2)\n",
    "model.to(args2.device)\n",
    "\n",
    "predictions = predict(args2, model, test_features)\n",
    "_output_bio(args2, test_example, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-penny",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! python3 \"/home/alexgre/workspace/py3/nlp/ClinicalTransformerNER/src/eval_scripts/new_bio_eval.py\"\\\n",
    "    -f1 /home/alexgre/shared_data/challenge_datasets/ner/clinical_ner_datasets/2012_i2b2/preprocessed_bio_data/test.txt \\\n",
    "    -f2 \"./i2b2_2012_megatron_bitfit_stage2_reserve/predicted.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-youth",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# only FT\n",
    "\n",
    "# temp_model = load_model(args1)\n",
    "\n",
    "args3 = Args(\n",
    "    model_type=\"megatron\", \n",
    "    pretrained_model=plm, \n",
    "    new_model_dir=\"./i2b2_2012_megatron_ft\", \n",
    "    data_dir=i2b2_2010,\n",
    "    do_predict = True,\n",
    "    logger_file=\"i2b2_2012_megatron_ft.txt\" \n",
    ")\n",
    "args3.device = torch.device(\"cuda:4\")\n",
    "args3.num_labels = num_labels\n",
    "args3.label2idx = label2idx\n",
    "args3.idx2label = idx2label\n",
    "args3.eval_tool = set_up_eval_tool(args3)\n",
    "args3.learning_rate = 2e-5\n",
    "args3.seed = 42\n",
    "\n",
    "train_examples = ner_data_processor.get_train_examples()\n",
    "train_features = transformer_convert_data_to_features(args3,\n",
    "                                                      input_examples=train_examples,\n",
    "                                                      label2idx=label2idx,\n",
    "                                                      tokenizer=tokenizer,\n",
    "                                                      max_seq_len=args3.max_seq_length)\n",
    "\n",
    "dev_examples = ner_data_processor.get_dev_examples()\n",
    "dev_features = transformer_convert_data_to_features(args3,\n",
    "                                                    input_examples=dev_examples,\n",
    "                                                    label2idx=label2idx,\n",
    "                                                    tokenizer=tokenizer,\n",
    "                                                    max_seq_len=args3.max_seq_length)\n",
    "\n",
    "# full FT\n",
    "conf_ft = BertConfig.from_pretrained(plm, num_labels=num_labels)\n",
    "model_ft = MegatronNerModel.from_pretrained(plm, config=conf_ft)\n",
    "model_ft.resize_token_embeddings(new_dim)\n",
    "conf_ft.vocab_size = new_dim\n",
    "args3.config = model_ft.config\n",
    "args3.tokenizer = tokenizer\n",
    "\n",
    "count_param(model_ft)    \n",
    "\n",
    "model_ft.to(args3.device)\n",
    "# start training\n",
    "train(args3, model_ft, train_features, dev_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-portland",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(args3.__dict__)\n",
    "test_example = ner_data_processor.get_test_examples()\n",
    "test_features = transformer_convert_data_to_features(args3,\n",
    "                                                     input_examples=test_example,\n",
    "                                                     label2idx=label2idx,\n",
    "                                                     tokenizer=args3.tokenizer,\n",
    "                                                     max_seq_len=args3.max_seq_length)\n",
    "\n",
    "model = load_model(args3)\n",
    "model.to(args3.device)\n",
    "\n",
    "predictions = predict(args3, model, test_features)\n",
    "_output_bio(args3, test_example, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "south-theater",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# eval\n",
    "! python3 \"/home/alexgre/workspace/py3/nlp/ClinicalTransformerNER/src/eval_scripts/new_bio_eval.py\"\\\n",
    "    -f1 /home/alexgre/shared_data/challenge_datasets/ner/clinical_ner_datasets/2012_i2b2/preprocessed_bio_data/test.txt \\\n",
    "    -f2 \"./i2b2_2012_megatron_ft/predicted.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-document",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "def get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, min_lr=1e-6, last_epoch=-1):\n",
    "    # this scheduler will use min_lr instead of 0\n",
    "    def lr_lambda(current_step: int):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        return max(\n",
    "            min_lr, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        )\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda, last_epoch)\n",
    "\n",
    "\n",
    "def train_mod(args, model, train_features, dev_features):\n",
    "    \"\"\"NER model training on train dataset; select model based on performance on dev dataset\"\"\"\n",
    "    # create data loader\n",
    "    data_loader = ner_data_loader(train_features, batch_size=args.train_batch_size, task='train', auto=True)\n",
    "    # total training step counts\n",
    "    t_total = len(data_loader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "\n",
    "    # parameters for optimization\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': args.weight_decay},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "\n",
    "    # using fp16 for training rely on Nvidia apex package\n",
    "    # fp16 training: try to use PyTorch naive implementation if available; we will only support apex anymore\n",
    "    scaler = None\n",
    "    autocast = None\n",
    "    if args.fp16:\n",
    "        try:\n",
    "            autocast = torch.cuda.amp.autocast\n",
    "            scaler = torch.cuda.amp.GradScaler()\n",
    "        except Exception:\n",
    "            raise ImportError(\"You need to update to PyTorch 1.6, the current PyTorch version is {}\"\n",
    "                              .format(torch.__version__))\n",
    "\n",
    "    # training linear warm-up setup\n",
    "    scheduler = None\n",
    "    if args.do_warmup:\n",
    "        warmup_steps = np.dtype('int64').type(args.warmup_ratio * t_total)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=warmup_steps, min_lr=args.min_lr, num_training_steps=t_total)\n",
    "\n",
    "    args.logger.info(\"***** Running training *****\")\n",
    "    args.logger.info(\"  Num data points = {}\".format(len(data_loader)))\n",
    "    args.logger.info(\"  Num Epochs = {}\".format(args.num_train_epochs))\n",
    "    args.logger.info(\"  Instantaneous batch size per GPU = {}\".format(args.train_batch_size))\n",
    "    args.logger.info(\"  Gradient Accumulation steps = {}\".format(args.gradient_accumulation_steps))\n",
    "    args.logger.info(\"  Total optimization steps = {}\".format(t_total))\n",
    "    args.logger.info(\"  Training steps (number of steps between two evaluation on dev) = {}\".format(\n",
    "        args.train_steps * args.gradient_accumulation_steps))\n",
    "    args.logger.info(\"******************************\")\n",
    "\n",
    "    # create directory to save model\n",
    "    new_model_dir = Path(args.new_model_dir)\n",
    "    new_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "    # save label2idx json in new model directory\n",
    "    json_dump(args.label2idx, new_model_dir / \"label2idx.json\")\n",
    "\n",
    "    # save base model name to a base_model_name.txt\n",
    "    with open(new_model_dir / \"base_model_name.txt\", \"w\") as f:\n",
    "        f.write('model_type: {}\\nbase_model: {}\\nconfig: {}\\ntokenizer: {}'.format(\n",
    "            args.model_type, args.pretrained_model, args.config_name, args.tokenizer_name))\n",
    "\n",
    "    global_step = 0\n",
    "    tr_loss = .0\n",
    "    best_score, epcoh_best_score = .0, .0\n",
    "    early_stop_flag = 0\n",
    "\n",
    "    set_freeze(model)\n",
    "    model.zero_grad()\n",
    "    \n",
    "    epoch_iter = trange(int(args.num_train_epochs), desc=\"Epoch\", disable=not args.progress_bar, position=1, leave=False)\n",
    "    for epoch in epoch_iter:\n",
    "        if epoch == 5:\n",
    "            set_unfreeze(model)\n",
    "        count_param(model) \n",
    "        batch_iter = tqdm(iterable=data_loader, desc='Batch', disable=not args.progress_bar)\n",
    "        for step, batch in enumerate(batch_iter):\n",
    "            model.train()\n",
    "\n",
    "            batch = tuple(b.to(args.device) for b in batch)\n",
    "            train_inputs = batch_to_model_inputs(batch, args.model_type)\n",
    "\n",
    "            if args.fp16:\n",
    "                with autocast():\n",
    "                    _, _, loss = model(**train_inputs)\n",
    "            else:\n",
    "                _, _, loss = model(**train_inputs)\n",
    "\n",
    "            loss = loss / args.gradient_accumulation_steps\n",
    "            tr_loss += loss.item()\n",
    "\n",
    "            if args.fp16:\n",
    "                scaler.scale(loss).backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            # apply ADVERSARIAL TRAINING\n",
    "            if args.adversarial_training:\n",
    "                adversarial_train(args, adversarial_trainer, model, train_inputs)\n",
    "\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                if args.fp16:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "                    optimizer.step()\n",
    "\n",
    "                if args.do_warmup:\n",
    "                    scheduler.step()\n",
    "\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "            # using training step\n",
    "            if args.train_steps > 0 and (global_step + 1) % args.train_steps == 0:\n",
    "                # the current implementation will skip the all evaluations in the first epoch\n",
    "                best_score, eval_loss = evaluate(\n",
    "                    args, model, new_model_dir, dev_features, epoch, global_step, best_score)\n",
    "                args.logger.info(\"\"\"\n",
    "                Global step: {}; \n",
    "                Epoch: {}; \n",
    "                average_train_loss: {:.4f}; \n",
    "                eval_loss: {:.4f}; \n",
    "                current best score: {:.4f}\"\"\".format(\n",
    "                    global_step, epoch + 1, round(tr_loss / global_step, 4), eval_loss, best_score))\n",
    "\n",
    "        # default model select method using strict F1-score with beta=1; evaluate model after each epoch on dev\n",
    "        if args.train_steps <= 0 or epoch == 0:\n",
    "            best_score, eval_loss = evaluate(\n",
    "                args, model, new_model_dir, dev_features, epoch, global_step, best_score)\n",
    "            args.logger.info(\"\"\"\n",
    "                Global step: {}; \n",
    "                Epoch: {}; \n",
    "                average_train_loss: {:.4f}; \n",
    "                eval_loss: {:.4f}; \n",
    "                current best score: {:.4f}\"\"\".format(\n",
    "                global_step, epoch + 1, round(tr_loss / global_step, 4), eval_loss, best_score))\n",
    "\n",
    "        # early stop check\n",
    "        if epcoh_best_score < best_score:\n",
    "            epcoh_best_score = best_score\n",
    "            early_stop_flag = 0\n",
    "        else:\n",
    "            early_stop_flag += 1\n",
    "\n",
    "        if 0 < args.early_stop <= early_stop_flag:\n",
    "            args.logger.warn('Early stop activated; performance not improve anymore.')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-cookie",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args3 = Args(\n",
    "    model_type=\"megatron\", \n",
    "    pretrained_model=plm, \n",
    "    new_model_dir=\"./i2b2_2012_megatron_ft_stage_bitfit\", \n",
    "    data_dir=i2b2_2010,\n",
    "    do_predict = True,\n",
    "    logger_file=\"i2b2_2012_megatron_ft_stage_bitfit.txt\" \n",
    ")\n",
    "args3.device = torch.device(\"cuda:4\")\n",
    "args3.num_labels = num_labels\n",
    "args3.label2idx = label2idx\n",
    "args3.idx2label = idx2label\n",
    "args3.eval_tool = set_up_eval_tool(args3)\n",
    "args3.learning_rate = 1e-5\n",
    "args3.seed = 13\n",
    "\n",
    "train_examples = ner_data_processor.get_train_examples()\n",
    "train_features = transformer_convert_data_to_features(args3,\n",
    "                                                      input_examples=train_examples,\n",
    "                                                      label2idx=label2idx,\n",
    "                                                      tokenizer=tokenizer,\n",
    "                                                      max_seq_len=args3.max_seq_length)\n",
    "\n",
    "dev_examples = ner_data_processor.get_dev_examples()\n",
    "dev_features = transformer_convert_data_to_features(args3,\n",
    "                                                    input_examples=dev_examples,\n",
    "                                                    label2idx=label2idx,\n",
    "                                                    tokenizer=tokenizer,\n",
    "                                                    max_seq_len=args3.max_seq_length)\n",
    "\n",
    "# full FT\n",
    "conf_ft = BertConfig.from_pretrained(plm, num_labels=num_labels)\n",
    "model_ft = MegatronNerModel.from_pretrained(plm, config=conf_ft)\n",
    "model_ft.resize_token_embeddings(new_dim)\n",
    "conf_ft.vocab_size = new_dim\n",
    "args3.config = model_ft.config\n",
    "args3.tokenizer = tokenizer \n",
    "\n",
    "model_ft.to(args3.device)\n",
    "count_param(model_ft)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-fifteen",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start training\n",
    "train_mod(args3, model_ft, train_features, dev_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-picture",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_example = ner_data_processor.get_test_examples()\n",
    "test_features = transformer_convert_data_to_features(args3,\n",
    "                                                     input_examples=test_example,\n",
    "                                                     label2idx=label2idx,\n",
    "                                                     tokenizer=args3.tokenizer,\n",
    "                                                     max_seq_len=args3.max_seq_length)\n",
    "\n",
    "model = load_model(args3)\n",
    "model.to(args3.device)\n",
    "\n",
    "predictions = predict(args3, model, test_features)\n",
    "_output_bio(args3, test_example, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-google",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# eval\n",
    "! python3 \"/home/alexgre/workspace/py3/nlp/ClinicalTransformerNER/src/eval_scripts/new_bio_eval.py\"\\\n",
    "    -f1 /home/alexgre/shared_data/challenge_datasets/ner/clinical_ner_datasets/2012_i2b2/preprocessed_bio_data/test.txt \\\n",
    "    -f2 \"./i2b2_2012_megatron_ft_stage_bitfit/predicted.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-probability",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python-alexgre",
   "language": "python",
   "name": "alexgre-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
